<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.png"/>
	<link rel="shortcut icon" href="/img/logo.png">
	
			    <title>
    Yeung's Page
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="yeung, cmu, stat, statml, carnegie mellon" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">YEUNG</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="Resume">
		                Resume
		            </a>
		        </li>
		        
		        <li>
		            <a href="/projects/" title="Projects">
		                Projects
		            </a>
		        </li>
		        
		        <li>
		            <a href="" title="Gallary">
		                Gallary
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/yyeung-lam" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="instagram" href="/null" target="_blank" rel="noopener">
                            <i class="icon fa fa-instagram"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="medium" href="https://medium.com/@yuyangli_93906" target="_blank" rel="noopener">
                            <i class="icon fa fa-medium"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://i.ytimg.com/vi/4ObVzTuFivY/maxresdefault.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >K-Nearest Neighbors</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul>
<li><input checked="" disabled="" type="checkbox"> Describe a dataset as points in a high dimensional space</li>
<li><input checked="" disabled="" type="checkbox"> Implement k-Nearest Neighbors with O(N) prediction</li>
<li><input checked="" disabled="" type="checkbox"> Describe the inductive bias of a k-NN classifier and relate<br>it to feature scale</li>
<li><input checked="" disabled="" type="checkbox"> Sketch the decision boundary for a learning algorithm<br>(compare k-NN and DT)</li>
<li><input checked="" disabled="" type="checkbox"> State Cover &amp; Hart (1967)’s large sample analysis of a<br>nearest neighbor classifier ?</li>
<li><input disabled="" type="checkbox"> Invent “new” k-NN learning algorithms capable of dealing<br>with even k</li>
<li><input checked="" disabled="" type="checkbox"> Explain computational and geometric examples of the<br>curse of dimensionality</li>
</ul>
<hr>
<p><strong>Def:</strong> Classification</p>
<ul>
<li>${(X,Y)}_i^N$</li>
<li>for all $i$, $X^i\in\mathbb{R}^M$</li>
<li>$Y^i$ labels</li>
</ul>
<p><strong>Def:</strong> Binary Classification</p>
<ul>
<li>But $Y_i\in{0, 1}$ or {True, False} …</li>
</ul>
<p><strong>Def:</strong> Hypothesis for Binary Classification.</p>
<ul>
<li>$h:\mathbb{R}^M \rightarrow {+, -}$</li>
<li>Train: learn $h$</li>
<li>Test: given $X$, predict $\hat{Y} = h(X)$</li>
</ul>
<hr>
<p><strong>Def:</strong> Nearest Neighbors Classifier</p>
<ul>
<li><p>def train(D): store D</p>
</li>
<li><p>def predict(X): assign the most common label of the nearest <em>k</em> points in D</p>
<ul>
<li>Measure distance (high dimensional):<ul>
<li>Euclidean distance: $g(u,v) = \sqrt{\sum_{m=1}^M (u_m-v_m)^2}$</li>
<li>Manhattan distance: $g(u,v) = \sum_{m=1}^M |u_m-v_m|$</li>
</ul>
</li>
<li>Tie:<ul>
<li>Remove furthest point</li>
<li>Weighted votes by distances</li>
<li>Another distance metric</li>
</ul>
</li>
<li>Inductive Bias:<ol>
<li>Similar points should have similar labels</li>
<li>All dimensions are created equally (feature scale could drastically affect) (KNN is likely to do poorly if data has lots of irrelevant features)<h4 id="Computational-complexity-N-training-examples-M-features"><a href="#Computational-complexity-N-training-examples-M-features" class="headerlink" title="Computational complexity: N training examples M features ?"></a>Computational complexity: N training examples M features ?</h4></li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>Task</th>
<th>Naive</th>
<th>k-d tree</th>
</tr>
</thead>
<tbody><tr>
<td>Train</td>
<td>O(1)</td>
<td>O(MNlogN)</td>
</tr>
<tr>
<td>Test</td>
<td>O(MN)</td>
<td>O(2^MlogN)</td>
</tr>
</tbody></table>
</li>
</ul>
<h4 id="Theoretical-guarantee"><a href="#Theoretical-guarantee" class="headerlink" title="Theoretical guarantee:"></a>Theoretical guarantee:</h4><p>Cover &amp; Hart (1967):<br>$$error_{true}(h) &lt; 2\text{ Bayes Error Rate}$$ as $$n\rightarrow \infty$$</p>
<ul>
<li>$h(x)$: Nearest Neighbors binary classifier (<em><strong>k=1</strong></em>)</li>
<li>$n$: number of training examples</li>
<li> Bayes Error Rate: “the best” you can guess</li>
<li>In other words,<blockquote>
<p>half the classification information in an infinite sample set is contained in the nearest neighbor.</p>
</blockquote>
</li>
</ul>
<h4 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h4><ul>
<li>Jagged decision boundary -&gt; overfitting</li>
<li>Simple decision boundary -&gt; underfitting</li>
</ul>
<h4 id="CIML-High-Dimensionality-is-scary"><a href="#CIML-High-Dimensionality-is-scary" class="headerlink" title="CIML: High Dimensionality is scary"></a>CIML: High Dimensionality is scary</h4><blockquote>
<p>The curse of dimensionality: computationally and mathematically</p>
</blockquote>
<hr>
<h3 id="KNN-Regression"><a href="#KNN-Regression" class="headerlink" title="KNN Regression"></a>KNN Regression</h3><p><img src="knn_reg.png"></p>
<p>Remark:</p>
<ul>
<li>Weighted:<ul>
<li>Train: store all (x, y) points</li>
<li>Predict: return weighted average of k nearest neighbors’ values</li>
<li>$k=2$: line that connects the points</li>
</ul>
</li>
<li>Unweighted: averaging k nearest neighbors’ y values</li>
</ul>
<p>Copyright (c) 2020 Copyright Yeung All Rights Reserved.</p>

            </div>

            <!-- Post Comments -->
            

        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2021 </span> 
			
        </div>
    </div>
</body>



 	
</html>
