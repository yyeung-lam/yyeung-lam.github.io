<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.png"/>
	<link rel="shortcut icon" href="/img/logo.png">
	
			    <title>
    Yeung's Page
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="yeung, cmu, stat, statml, carnegie mellon" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">YEUNG</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="Resume">
		                Resume
		            </a>
		        </li>
		        
		        <li>
		            <a href="/projects/" title="Projects">
		                Projects
		            </a>
		        </li>
		        
		        <li>
		            <a href="" title="Gallary">
		                Gallary
		            </a>
		        </li>
		        
		        <li>
		            <a href="/portfolio/" title="Portfolio">
		                Portfolio
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/yyeung-lam" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="instagram" href="/null" target="_blank" rel="noopener">
                            <i class="icon fa fa-instagram"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="medium" href="https://yyeung-lam.medium.com/" target="_blank" rel="noopener">
                            <i class="icon fa fa-medium"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://miro.medium.com/max/2400/1*LVM828XprIIDbled8ET4XQ.gif);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >PCA</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h2 id="PCA-Dimensionality-Reduction"><a href="#PCA-Dimensionality-Reduction" class="headerlink" title="PCA: Dimensionality Reduction"></a>PCA: Dimensionality Reduction</h2><h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul>
<li><input disabled="" type="checkbox"> Define the sample mean, sample variance, and sample covariance of a vector Ǧvalued dataset</li>
<li><input disabled="" type="checkbox"> Identify examples of high dimensional data and common use cases for dimensionality reduction</li>
<li><input disabled="" type="checkbox"> Draw the principal components of a given toy dataset</li>
<li><input disabled="" type="checkbox"> Establish the equivalence of minimization of reconstruction error with maximization of variance</li>
<li><input disabled="" type="checkbox"> Given a set of principal components, project from high to low dimensional space and do the reverse to produce a<br>reconstruction</li>
<li><input disabled="" type="checkbox"> Explain the connection between PCA, eigenvectors, eigenvalues, and covariance matrix</li>
<li><input disabled="" type="checkbox"> Use common methods in linear algebra to obtain the principal components</li>
</ul>
<h4 id="Learning-Representation"><a href="#Learning-Representation" class="headerlink" title="Learning Representation"></a>Learning Representation</h4><p><strong>PCA, Kernel PCA, ICA:</strong> unsupervised learning techniques for extracting hidden (low dimensional) structural from high dimensional datasets</p>
<ul>
<li>more efficient</li>
<li>better generalization</li>
<li>noise removal</li>
<li>further processing for other machine learning algorithms</li>
</ul>
<p><strong>Goal:</strong></p>
<p><strong>Data:</strong> $\mathcal{D} = {\mathbf{x}^{(i)}}<em>{i=1}^N$, arrange data into matrix $\mathbf{X} = \begin{bmatrix}(\mathbf{x}^{(1)})^T\ …\<br>(\mathbf{x}^{(N)})^T\end{bmatrix}$ and is centered: $\mu = \frac{1}{N}\sum</em>{i=1}^N\mathbf{x}^{(i)} = \vec0$<br><strong>Assumption</strong> If data is not centered, subtract the mean of that column.  $\mu$ from $\mathbf{x}^{(i)}$. Be aware that features lie vertically<br><strong>Sample Covariate Matrix:</strong> $\Sigma_{j,k} = \frac{1}{N}\sum_{i=1}^N(x_j^{(i)}-\mu_j)(x_k^{(i)}-\mu_k)$ and we ca rewrite as $$\mathbf{\Sigma} = \frac{1}{N}\mathbf{X}^T\mathbf{X}$$</p>
<h4 id="PCA-Algorithm"><a href="#PCA-Algorithm" class="headerlink" title="PCA Algorithm"></a>PCA Algorithm</h4><p><strong>Given</strong> $K$ vectors $\vec v_1,…,\vec v_K$ where $\vec v\in\mathbb{R}^M$, the projection of a vector $\vec x^{(i)}\in\mathbb{R}^M$ to a lower $K$-dimensional space is $\vec u^{(i)}\in\mathbb{R}^K$ where<br>$$\vec u^{(i)} = \begin{bmatrix}\vec v_1^T\vec x^{(i)}\ … \ \vec v_K^T\vec x^{(i)} \end{bmatrix} = \mathbf{V}^T\vec x^{(i)}$$<br>where $\mathbf{V} = \begin{bmatrix}\vec v_1^T\…\ \vec v_K^T \end{bmatrix}$</p>
<p><strong>PCA repeatedly chooses next $\vec v_j$ that minimizes the reconstruction error such that $\vec v_j$ is orthogonal to $\vec v_1,…,\vec v_{j-1}$</strong> $\Rightarrow$ $K$-dimensions in PCA are uncorrelated, any information about one dimension is not going to provide any information in another dimension</p>
<blockquote>
<p>Recall the length of projection of $\vec x$ onto $\vec v$ is $a = \frac{\vec v^T\vec x}{|v|_2}$, the projection of $\vec x$ onto $\vec v$ is<br>$\vec u = \vec{v} a = \frac{(\vec v^T\vec x)\vec v}{|v|_2}$</p>
</blockquote>
<p><strong>Objective Functions</strong></p>
<ol>
<li><p>Minimizing the reconstruction error<br>$$\vec v = \argmin_{\vec v} \frac{1}{N}\sum_{i=1}^N|\vec x^{(i)} - proj(\vec v^{(i)})|^2_2$$<br>$$= \argmin_{\vec v} \frac{1}{N}\sum_{i=1}^N|\vec x^{(i)} - \vec v^T\vec x^{(i)}\vec v|^2_2 \tag{suppose $|\vec v|^1_2 = 1$}$$</p>
</li>
<li><p>Maximizing the variance of the points in lower dimensional space: the sum of length of projection to the mean<br>$$\vec v = \argmax_{\vec v} \frac{1}{N}\sum_{i=1}^N(\vec v^T\vec x^{(i)})^2 \tag{suppose $|\vec v|^1_2 = 1$}$$</p>
</li>
<li><p>Two objective functions are equivalent. Rewrite in matrix form: $$\vec = \argmax_{\vec v}\frac{1}{N}\vec v^T(\mathbf{X}^T\mathbf{X})\vec v = \argmax_{\vec v}\vec v^T\mathbf{\Sigma}\vec v$$</p>
</li>
</ol>
<blockquote>
<p>Background: Eigenvectors and Eigenvalues<br>For square matrix $\mathbf{A}$, the (column) vector $\vec v$ is an eigenvector iff there exists eigenvalue $\lambda$ such that $\mathbf{A}\vec v = \lambda\vec v$ i.e, The linear transformation of $\mathbf{A}$ is only stretching $\vec v$.</p>
</blockquote>
<p><strong>Theorem 1:</strong> The vector that maximizing variance is the eigenvector (a column vector of shape $M\times 1$) of $\mathbf{\Sigma}$ with largest eigenvalue.<br><strong>Theorem 2:</strong> Eigenvectors of symmetric matrix is orthogonal to each other.<br><strong>Fact:</strong> $\mathbf{\Sigma}$ is a symmetric matrix</p>
<p>PCA projections $\vec u^{(i)} = \begin{bmatrix}\vec v_1^T\vec x^{(i)}\ … \ \vec v_K^T\vec x^{(i)} \end{bmatrix} = \mathbf{V}^T\vec x^{(i)}$ so that $\vec v_{k}$ is the eigenvector with the $k^{th}$ largest eigenvalue</p>
<h4 id="Algorithms-to-get-Eigenvectors-for-PCA"><a href="#Algorithms-to-get-Eigenvectors-for-PCA" class="headerlink" title="Algorithms to get Eigenvectors for PCA"></a>Algorithms to get Eigenvectors for PCA</h4><ul>
<li>Power iteration (aka. Von Mises iteration)<ul>
<li>Finds each principal component one at a time</li>
<li>$\mathbf{A} = \mathbf{U}\mathbf{\Delta}\mathbf{V}^T$ where $\mathbf{\Delta}$ is diagonal and $\mathbf{U},\mathbf{V}$ are orthogonal</li>
</ul>
</li>
<li>Singular Value Decomposition (SVD)<ul>
<li>Finds all principal components at once</li>
</ul>
</li>
<li>Stochastic Methods (approximate)<ul>
<li>Very efficient</li>
</ul>
</li>
</ul>
<h4 id="Number-of-PCs"><a href="#Number-of-PCs" class="headerlink" title="Number of PCs?"></a>Number of PCs?</h4><ul>
<li>There are up to $M$ many PCs for $\mathbf{\Sigma}\in\mathbb{R}^{M\times M}$</li>
<li>To reduce dimensionality, we want fewer PCs</li>
<li>To avoid losing information, we choose PCs that has large eigenvalues, and ignore the components of less significance<ul>
<li>The value of eigenvalue is correlated with the amount of information carried on that eigenvector</li>
</ul>
</li>
<li>e.g, selects PCs with variance ratio higher than $5%$</li>
</ul>
<p>Copyright (c) 2020 Copyright Yeung All Rights Reserved.</p>

            </div>

            <!-- Post Comments -->
            

        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2021 </span> 
			
        </div>
    </div>
</body>



 	
</html>
